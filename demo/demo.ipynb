{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/donglianghan/opt/anaconda3/envs/janus/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version is above 3.10, patching the collections module.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/donglianghan/opt/anaconda3/envs/janus/lib/python3.10/site-packages/transformers/models/auto/image_processing_auto.py:590: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import torch\n",
    "from transformers import AutoConfig, AutoModelForCausalLM\n",
    "from janus.models import MultiModalityCausalLM, VLChatProcessor\n",
    "from janus.utils.io import load_pil_images\n",
    "from demo.cam import generate_gradcam\n",
    "from captum.attr import LayerGradCam\n",
    "from PIL import Image\n",
    "from einops import rearrange\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n",
      "Some kwargs in processor config are unused and will not have any effect: sft_format, add_special_token, image_tag, mask_prompt, ignore_id, num_image_tokens. \n"
     ]
    }
   ],
   "source": [
    "torch.set_default_device(\"mps\")\n",
    "model_path = \"deepseek-ai/Janus-Pro-1B\"\n",
    "config = AutoConfig.from_pretrained(model_path)\n",
    "language_config = config.language_config\n",
    "language_config._attn_implementation = 'eager'\n",
    "vl_gpt = AutoModelForCausalLM.from_pretrained(model_path,\n",
    "                                             language_config=language_config,\n",
    "                                             trust_remote_code=True)\n",
    "\n",
    "dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float16\n",
    "# dtype = torch.bfloat32 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    vl_gpt = vl_gpt.to(dtype).cuda()\n",
    "else:\n",
    "    # vl_gpt = vl_gpt.to(torch.float16)\n",
    "    vl_gpt = vl_gpt.to(dtype)\n",
    "\n",
    "vl_chat_processor = VLChatProcessor.from_pretrained(model_path)\n",
    "tokenizer = vl_chat_processor.tokenizer\n",
    "cuda_device = 'cuda' if torch.cuda.is_available() else 'mps'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multimodal_understanding(image, question, seed, top_p, temperature):\n",
    "    # Clear CUDA cache before generating\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "    for param in vl_gpt.parameters():\n",
    "        param.requires_grad = True\n",
    "    \n",
    "    # set seed\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    \n",
    "\n",
    "    # Get the last transformer block of the Vision Transformer (ViT)\n",
    "\n",
    "\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"<|User|>\",\n",
    "            \"content\": f\"<image_placeholder>\\n{question}\",\n",
    "            \"images\": [image],\n",
    "        },\n",
    "        {\"role\": \"<|Assistant|>\", \"content\": \"\"},\n",
    "    ]\n",
    "    \n",
    "    pil_images = [Image.fromarray(image)]\n",
    "    prepare_inputs = vl_chat_processor(\n",
    "        conversations=conversation, images=pil_images, force_batchify=True\n",
    "    ).to(cuda_device, dtype=dtype)\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)\n",
    "\n",
    "    # print(\"prepared inputs\", prepare_inputs)\n",
    "    \n",
    "\n",
    "    outputs = vl_gpt.language_model.generate(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=prepare_inputs.attention_mask,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=False if temperature == 0 else True,\n",
    "        use_cache=True,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "    )\n",
    "\n",
    "\n",
    "    \n",
    "    answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)\n",
    "\n",
    "\n",
    "\n",
    "    print(\"generating guided gradcam...\")\n",
    "\n",
    "    import torch.nn as nn\n",
    "\n",
    "    class ViTForGradCAM(nn.Module):\n",
    "        def __init__(self, vision_model):\n",
    "            super().__init__()\n",
    "            self.vision_model = vision_model\n",
    "\n",
    "        def forward(self, images):\n",
    "            # Get the output from your ViT model.\n",
    "            # Suppose the output shape is [batch, T2, D]\n",
    "            outputs = self.vision_model(images)\n",
    "            \n",
    "            # Select the [CLS] token (assuming it's at index 0)\n",
    "            cls_token = outputs[:, 0, :]  # shape: [batch, D]\n",
    "            \n",
    "            # Now, reduce the vector to a scalar.\n",
    "            # Option 1: Simply take one element, e.g. the first element:\n",
    "            scalar_output = cls_token[:, 0]  # shape: [batch]\n",
    "            \n",
    "            # Option 2: Or aggregate, for example using a linear layer or a pooling operation:\n",
    "            # scalar_output = cls_token.mean(dim=1)  # shape: [batch]\n",
    "            \n",
    "            return scalar_output\n",
    "\n",
    "    # Wrap your vision model\n",
    "    vit_scalar_model = ViTForGradCAM(vl_gpt.vision_model)\n",
    "    target_layer = vit_scalar_model.vision_model.vision_tower.blocks[-1].norm1\n",
    "\n",
    "    bs, n = prepare_inputs.pixel_values.shape[0:2]\n",
    "    images = rearrange(prepare_inputs.pixel_values, \"b n c h w -> (b n) c h w\")\n",
    "    # [b x n, T2, D]\n",
    "    images_embeds = vit_scalar_model(images)\n",
    "\n",
    "    guided_gc = LayerGradCam(vit_scalar_model, layer=target_layer)\n",
    "    print(\"generating attribute...\")\n",
    "    attribution = guided_gc.attribute(\n",
    "        images,\n",
    "        # target=0\n",
    "    )\n",
    "    print(\"generating saliency map...\")\n",
    "    saliency_map = generate_gradcam(attribution, pil_images[0])\n",
    "\n",
    "    # return answer, [saliency_map]\n",
    "    plt.imshow(saliency_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "janus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
