{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/donglianghan/opt/anaconda3/envs/janus/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version is above 3.10, patching the collections module.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/donglianghan/opt/anaconda3/envs/janus/lib/python3.10/site-packages/transformers/models/auto/image_processing_auto.py:590: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import torch\n",
    "from transformers import AutoConfig, AutoModelForCausalLM\n",
    "from janus.models import MultiModalityCausalLM, VLChatProcessor\n",
    "from janus.utils.io import load_pil_images\n",
    "from demo.cam import generate_gradcam\n",
    "from captum.attr import LayerGradCam\n",
    "from PIL import Image\n",
    "from einops import rearrange\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n",
      "Some kwargs in processor config are unused and will not have any effect: sft_format, add_special_token, image_tag, mask_prompt, ignore_id, num_image_tokens. \n"
     ]
    }
   ],
   "source": [
    "torch.set_default_device(\"mps\")\n",
    "model_path = \"deepseek-ai/Janus-Pro-1B\"\n",
    "config = AutoConfig.from_pretrained(model_path)\n",
    "language_config = config.language_config\n",
    "language_config._attn_implementation = 'eager'\n",
    "vl_gpt = AutoModelForCausalLM.from_pretrained(model_path,\n",
    "                                             language_config=language_config,\n",
    "                                             trust_remote_code=True)\n",
    "\n",
    "dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float16\n",
    "# dtype = torch.bfloat32 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    vl_gpt = vl_gpt.to(dtype).cuda()\n",
    "else:\n",
    "    # vl_gpt = vl_gpt.to(torch.float16)\n",
    "    vl_gpt = vl_gpt.to(dtype)\n",
    "\n",
    "vl_chat_processor = VLChatProcessor.from_pretrained(model_path)\n",
    "tokenizer = vl_chat_processor.tokenizer\n",
    "cuda_device = 'cuda' if torch.cuda.is_available() else 'mps'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multimodal_understanding(image, question, seed, top_p, temperature):\n",
    "    # Clear CUDA cache before generating\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "    for param in vl_gpt.parameters():\n",
    "        param.requires_grad = True\n",
    "    \n",
    "    # set seed\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    \n",
    "\n",
    "    # Get the last transformer block of the Vision Transformer (ViT)\n",
    "\n",
    "\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"<|User|>\",\n",
    "            \"content\": f\"<image_placeholder>\\n{question}\",\n",
    "            \"images\": [image],\n",
    "        },\n",
    "        {\"role\": \"<|Assistant|>\", \"content\": \"\"},\n",
    "    ]\n",
    "    \n",
    "    pil_images = [Image.fromarray(image)]\n",
    "    prepare_inputs = vl_chat_processor(\n",
    "        conversations=conversation, images=pil_images, force_batchify=True\n",
    "    ).to(cuda_device, dtype=dtype)\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)\n",
    "\n",
    "    # print(\"prepared inputs\", prepare_inputs)\n",
    "    \n",
    "\n",
    "    outputs = vl_gpt.language_model.generate(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=prepare_inputs.attention_mask,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=False if temperature == 0 else True,\n",
    "        use_cache=True,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "    )\n",
    "\n",
    "\n",
    "    \n",
    "    answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)\n",
    "\n",
    "\n",
    "\n",
    "    print(\"generating guided gradcam...\")\n",
    "\n",
    "    import torch.nn as nn\n",
    "\n",
    "    class ViTForGradCAM(nn.Module):\n",
    "        def __init__(self, vision_model):\n",
    "            super().__init__()\n",
    "            self.vision_model = vision_model\n",
    "\n",
    "        def forward(self, images):\n",
    "            # Get the output from your ViT model.\n",
    "            # Suppose the output shape is [batch, T2, D]\n",
    "            outputs = self.vision_model(images)\n",
    "            \n",
    "            # Select the [CLS] token (assuming it's at index 0)\n",
    "            cls_token = outputs[:, 0, :]  # shape: [batch, D]\n",
    "            \n",
    "            # Now, reduce the vector to a scalar.\n",
    "            # Option 1: Simply take one element, e.g. the first element:\n",
    "            scalar_output = cls_token[:, 0]  # shape: [batch]\n",
    "            \n",
    "            # Option 2: Or aggregate, for example using a linear layer or a pooling operation:\n",
    "            # scalar_output = cls_token.mean(dim=1)  # shape: [batch]\n",
    "            \n",
    "            return scalar_output\n",
    "\n",
    "    # Wrap your vision model\n",
    "    vit_scalar_model = ViTForGradCAM(vl_gpt.vision_model)\n",
    "    target_layer = vit_scalar_model.vision_model.vision_tower.blocks[-1].norm1\n",
    "\n",
    "    bs, n = prepare_inputs.pixel_values.shape[0:2]\n",
    "    images = rearrange(prepare_inputs.pixel_values, \"b n c h w -> (b n) c h w\")\n",
    "    # [b x n, T2, D]\n",
    "    images_embeds = vit_scalar_model(images)\n",
    "\n",
    "    guided_gc = LayerGradCam(vit_scalar_model, layer=target_layer)\n",
    "    print(\"generating attribute...\")\n",
    "    attribution = guided_gc.attribute(\n",
    "        images,\n",
    "        # target=0\n",
    "    )\n",
    "    print(\"generating saliency map...\")\n",
    "    saliency_map = generate_gradcam(attribution, pil_images[0])\n",
    "\n",
    "    # return answer, [saliency_map]\n",
    "    plt.imshow(saliency_map)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "torch.cat(): all input tensors must be on the same device. Received mps:0 and cpu",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m image \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(image)\n\u001b[1;32m      3\u001b[0m question \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexplain this meme.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mmultimodal_understanding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.95\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 28\u001b[0m, in \u001b[0;36mmultimodal_understanding\u001b[0;34m(image, question, seed, top_p, temperature)\u001b[0m\n\u001b[1;32m     18\u001b[0m conversation \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     19\u001b[0m     {\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<|User|>\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<|Assistant|>\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m     25\u001b[0m ]\n\u001b[1;32m     27\u001b[0m pil_images \u001b[38;5;241m=\u001b[39m [Image\u001b[38;5;241m.\u001b[39mfromarray(image)]\n\u001b[0;32m---> 28\u001b[0m prepare_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mvl_chat_processor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconversations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconversation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpil_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_batchify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     30\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(cuda_device, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m     35\u001b[0m inputs_embeds \u001b[38;5;241m=\u001b[39m vl_gpt\u001b[38;5;241m.\u001b[39mprepare_inputs_embeds(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mprepare_inputs)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# print(\"prepared inputs\", prepare_inputs)\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/My_Projects/Janus/janus/models/processing_vlm.py:348\u001b[0m, in \u001b[0;36mVLChatProcessor.__call__\u001b[0;34m(self, prompt, conversations, images, force_batchify, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    330\u001b[0m ):\n\u001b[1;32m    331\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    332\u001b[0m \n\u001b[1;32m    333\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;124;03m            - num_image_tokens (List[int]): the number of image tokens\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 348\u001b[0m     prepare \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_one\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconversations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconversations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimages\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    352\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m force_batchify:\n\u001b[1;32m    353\u001b[0m         prepare \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatchify([prepare])\n",
      "File \u001b[0;32m~/Desktop/My_Projects/Janus/janus/models/processing_vlm.py:305\u001b[0m, in \u001b[0;36mVLChatProcessor.process_one\u001b[0;34m(self, prompt, conversations, images, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m image_token_mask: torch\u001b[38;5;241m.\u001b[39mBoolTensor \u001b[38;5;241m=\u001b[39m input_ids \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_id\n\u001b[1;32m    304\u001b[0m image_indices \u001b[38;5;241m=\u001b[39m image_token_mask\u001b[38;5;241m.\u001b[39mnonzero()\n\u001b[0;32m--> 305\u001b[0m input_ids, num_image_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_image_token\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;66;03m# load images\u001b[39;00m\n\u001b[1;32m    311\u001b[0m images_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_processor(images, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/My_Projects/Janus/janus/models/processing_vlm.py:255\u001b[0m, in \u001b[0;36mVLChatProcessor.add_image_token\u001b[0;34m(self, image_indices, input_ids)\u001b[0m\n\u001b[1;32m    252\u001b[0m input_slices\u001b[38;5;241m.\u001b[39mappend(input_ids[start:])\n\u001b[1;32m    254\u001b[0m \u001b[38;5;66;03m# concat all slices\u001b[39;00m\n\u001b[0;32m--> 255\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_slices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m num_image_tokens \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mIntTensor([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_image_tokens] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(image_indices))\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m input_ids, num_image_tokens\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/janus/lib/python3.10/site-packages/torch/utils/_device.py:77\u001b[0m, in \u001b[0;36mDeviceContext.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m _device_constructors() \u001b[38;5;129;01mand\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: torch.cat(): all input tensors must be on the same device. Received mps:0 and cpu"
     ]
    }
   ],
   "source": [
    "image = Image.open(\"../images/doge.png\")\n",
    "image = np.array(image)\n",
    "question = \"explain this meme.\"\n",
    "multimodal_understanding(image, question, 100, 0.95, 0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "janus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
